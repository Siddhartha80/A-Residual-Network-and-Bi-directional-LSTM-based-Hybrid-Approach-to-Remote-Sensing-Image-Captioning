{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17d1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SAI REVANTH\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"dark\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17411133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           image                                            caption\n",
      "0  airport_1.jpg  many planes are parked next to a long building...\n",
      "1  airport_1.jpg  many planes are parked next to a long building...\n",
      "2  airport_1.jpg  many planes are parked next to a long building...\n",
      "3  airport_1.jpg  many planes are parked next to a long building...\n",
      "4  airport_1.jpg  many planes are parked next to a long building...\n"
     ]
    }
   ],
   "source": [
    "# Read the Excel file\n",
    "data = pd.read_excel(r\"C:\\Users\\SAI REVANTH\\Downloads\\Modified_Book2.xlsx\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa41e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\SAI REVANTH\\Downloads\\ASEB 2021-25\\RSICD_images\\RSICD_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a1fb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImage(path,img_size=224):\n",
    "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    return img\n",
    "\n",
    "def display_images(temp_df):\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    plt.figure(figsize = (20 , 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n+=1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
    "        image = readImage(r\"C:\\Users\\SAI REVANTH\\Downloads\\ASEB 2021-25\\RSICD_images\\RSICD_images{temp_df.image[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d71488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_images(data.sample(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65ba66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
    "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
    "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
    "    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35da5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq many planes are parked next to long building in an airport endseq',\n",
       " 'startseq many planes are parked next to long building in an airport endseq',\n",
       " 'startseq many planes are parked next to long building in an airport endseq',\n",
       " 'startseq many planes are parked next to long building in an airport endseq',\n",
       " 'startseq many planes are parked next to long building in an airport endseq',\n",
       " 'startseq some planes are parked in an airport endseq',\n",
       " 'startseq the airport here is full of airplanes and containers endseq',\n",
       " 'startseq the airport here is full of airplanes and containers endseq',\n",
       " 'startseq some planes are parked in an airport endseq',\n",
       " 'startseq some planes are parked in an airport endseq']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = text_preprocessing(data)\n",
    "captions = data['caption'].tolist()\n",
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ecbd1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 66, 3, 45, 44, 40, 119, 35, 6, 29, 52, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in captions)\n",
    "\n",
    "images = data['image'].unique().tolist()\n",
    "nimages = len(images)\n",
    "\n",
    "split_index = round(0.85*nimages)\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "train = data[data['image'].isin(train_images)]\n",
    "test = data[data['image'].isin(val_images)]\n",
    "\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "tokenizer.texts_to_sequences([captions[1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "367c2c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10921/10921 [1:43:47<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "img_size = 224\n",
    "features = {}\n",
    "for image in tqdm(data['image'].unique().tolist()):\n",
    "    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    feature = fe.predict(img, verbose=0)\n",
    "    features[image] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ec7be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n",
    "                 vocab_size, max_length, features,shuffle=True):\n",
    "    \n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.features = features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "    \n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n",
    "        X1, X2, y = self.__get_data(batch)        \n",
    "        return (X1, X2), y\n",
    "    \n",
    "    def __get_data(self,batch):\n",
    "        \n",
    "        X1, X2, y = list(), list(), list()\n",
    "        \n",
    "        images = batch[self.X_col].tolist()\n",
    "           \n",
    "        for image in images:\n",
    "            feature = self.features[image][0]\n",
    "            \n",
    "            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                for i in range(1,len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            \n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "                \n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3342993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SAI REVANTH\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(1920,))\n",
    "input2 = Input(shape=(max_length,))\n",
    "\n",
    "img_features = Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
    "sentence_features = LSTM(256)(merged)\n",
    "x = Dropout(0.5)(sentence_features)\n",
    "x = add([x, img_features])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "caption_model = Model(inputs=[input1,input2], outputs=output)\n",
    "caption_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86269249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51600210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3dfc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 1920)]               0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  491776    ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 34)]                 0         []                            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 256)               0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 34, 256)              763392    ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 35, 256)              0         ['reshape[0][0]',             \n",
      "                                                                     'embedding[0][0]']           \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 256)                  525312    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 256)                  0         ['dropout[0][0]',             \n",
      "                                                                     'dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 128)                  32896     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 128)                  0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 2982)                 384678    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2198054 (8.38 MB)\n",
      "Trainable params: 2198054 (8.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "315ba8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
    "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n",
    "\n",
    "validation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n",
    "                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52f0d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model.keras\"  # Corrected filepath with .keras extension\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', \n",
    "                              min_delta=0, \n",
    "                              patience=5, \n",
    "                              verbose=1, \n",
    "                              restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.2, \n",
    "                                            min_lr=0.00000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = caption_model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint,earlystopping,learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d210d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71398061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754c56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image, tokenizer, max_length, features):\n",
    "    \n",
    "    feature = features[image]\n",
    "    in_text = \"startseq\"\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "\n",
    "        y_pred = model.predict([feature,sequence])\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        \n",
    "        word = idx_to_word(y_pred, tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        in_text+= \" \" + word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "    return in_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eead0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test.sample(15)\n",
    "samples.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460cf7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,record in samples.iterrows():\n",
    "\n",
    "    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n",
    "    samples.loc[index,'caption'] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853e04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
